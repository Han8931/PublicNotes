Reference: Django for Professionals

# Docker

An analogy we can use here is that of homes and apartments. Virtual Machines are like homes: stand-alone buildings with their own infrastructure including plumbing and heating, as well as a kitchen, bathrooms, bedrooms, and so on. Docker containers are like apartments: they share common infrastructure like plumbing and heating, but come in various sizes that match the exact needs of an owner.

## Virtual Environment vs Containers

_Virtual environments are a way to isolate Python packages_. Thanks to virtual environments, one computer can run multiple projects locally. For example, Project A might use Python 3.10 and Django 4.0 among other dependencies; whereas Project B uses Python 3.5 and Django 1.11. By creating a new virtual environment for each project and installing Python packages into it, rather than globally on the computer itself, all the necessary Python packages can be maintained, managed, and updated as needed.

There are several ways to implement virtual environments but arguably the simplest is with the `venv` module already installed as part of the Python 3 standard library. We will use `venv` shortly to install Django locally on the computer.

The important distinction between virtual environments and Docker containers is that 
1. __virtual environments can only isolate Python packages.__ They cannot isolate non-Python software like a PostgreSQL or MySQL database. 
2. The virtual environments still rely on a global, system-level installation of Python (in other words, on your computer).   

_Linux containers go a step further and isolate the entire operating system, not just the Python parts_. In other words, we will install Python itself within Docker as well as install and run a production-level database.

Docker itself is a complex topic and we won't dive that deep into it in this book, however understanding its background and key components is important. If you’d like to learn more about it, I recommend the Dive into Docker video course16.

## Docker Image

_A Docker image is a read-only template that describes how to create a Docker container._ The image is the instructions while the container is the actual running instance of an image. To continue our apartment analogy from earlier in the chapter, an image is the blueprint or set of plans for building an apartment; the container is the actual, fully-built building.

Images are often based on another image with some additional customization. For example, there is a long list of officially supported images for Python depending on the version and flavor of Python desired.

## Dockerfile

For our Django project we need to create a custom image that contains Python but also installs our code and has additional configuration details. _To build our own image we create a special file known as a Dockerfile that defines the steps to create and run the custom image._

Use your text editor to create a new Dockerfile file in the project-level directory next to the manage.py file. Within it add the following code which we'll walk through line-by-line below.

```Dockerfile
# Pull base image
FROM python:3.10.4-slim-bullseye
# Set environment variables
ENV PIP_DISABLE_PIP_VERSION_CHECK 1
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
# Set work directory
WORKDIR /code
# Install dependencies
COPY ./requirements.txt .
RUN pip install -r requirements.txt
# Copy project
COPY . .
```

Dockerfiles are read from top-to-bottom when an image is created. The first instruction is a `FROM` command that tells Docker what base image we would like to use for our application. Docker images can be inherited from other images so instead of creating our own base image, we'll use the official Python image that already has all the tools and packages that we need for our Django application. In this case we're using Python 3.10.4 and the much smaller in size slim variant that does not contain the common packages contained in the default tag. The tag bullseye refers to the latest stable Debian release. It is a good idea to set this explicitly to minimize potential breakage when there are new releases of Debian.

Then we use the `ENV` command to set three environment variables:
- `PIP_DISABLE_PIP_VERSION_CHECK` disables an automatic check for pip updates each time
- `PYTHONDONTWRITEBYTECODE` means Python will not try to write `.pyc` files
- `PYTHONUNBUFFERED` ensures our console output is not buffered by Docker

The command `WORKDIR` is used to set a default working directory when running the rest of our commands. This tells Docker to use this path as the default location for all subsequent commands. As a result, we can use relative paths based on the working directory rather than typing out the full file path each time. In our case the working directory is `/code` but it can often be much longer and something like `/app/src`, `/usr/src/app`, or similar variations depending upon the specific needs of a project.

The next step is to install our dependencies with `pip` and the `requirements.txt` file we already created. The `COPY` command takes two parameters: 
- The first parameter tells Docker what file(s) to copy into the image  
- The second parameter tells Docker where you want the file(s) to be copied to. 
In this case we are copying the existing `requirements.txt` file from our local computer into the current working directory which is represented by `.` . 

Once the `requirements.txt` file is inside the image we can use our last command, `RUN`, to execute `pip install`. This works exactly the same as if we were running pip install locally on our machine, but this time the modules are installed into the image. The `-r` flag tells pip to open a file–called `requirements.txt` here–and install its contents. If we did not include the -r flag pip would try and fail to install requirements.txt since it isn't itself an actual Python package.

At the moment we have a new image based on the slim-bullseye variant of Python 3.10.4 and have installed our dependencies. The final step is to copy all the files in our current directory into the working directory on the image. We can do this by using the `COPY` command. Remember it takes two parameters so we'll copy the current directory on our local filesystem (.) into the working directory (.) of the image.

If you're confused right now don’t worry. Docker is a lot to absorb but the good news is that the steps involved to "Dockerize" an existing project are very similar.

## .dockerignore

A `.dockerignore` file is a best practice way to specify certain files and directories that should not be included in a Docker image. This can help reduce overall image size and improves security by keeping things that are meant to be secret out of Docker.

At the moment we can safely ignore the local virtual environment (`.venv`), our future .git directory, and a `.gitignore` file. In your text editor create a new file called `.dockerignore` in the base directory next to the existing `manage.py` file.

We now have our complete instructions for creating a custom image but we haven’t actually built it yet. The command to do this is unsurprisingly `docker build` followed by the period, `.`, indicating the Dockerfile is located in the current directory. There will be a lot of output here. I've only included the first two lines and the last one.

```sh
docker build .
```

## Docker Compose

Our fully-built custom image is now available to run as a container. In order to run the container we need a list of instructions in a file called `docker-compose.yml`. With your text editor create a `docker-compose.yml` file in the project-level directory next to the `Dockerfile`. It will contain the following code.

```yml
version: "3.9"
services:
  web:
    build: .
    ports:
      - "8000:8000"
    command: python manage.py runserver 0.0.0.0:8000
    volumes:
      - .:/code  
```

- On the top line we set the most recent version of Docker Compose which is currently 3.9. 
- Then we specify which services (or containers) we want to have running within our Docker host. It's possible to have multiple services running, but for now we just have one for web.
- Within web we set build to look in the current directory for our `Dockerfile`. 
- We'll use the Django default ports of 8000. 
- Then, we execute the command to run the local web server. 
- Finally the volumes mount automatically syncs the Docker filesystem with our local computer's filesystem. If we make a change to the code within Docker it will automatically be synced with the local filesystem.

The final step is to run our Docker container using the command docker-compose up. This command will result in another long stream of output code on the command line.

```sh
docker-compose up
```

To confirm it actually worked, go back to `http://127.0.0.1:8000/` in your web browser. Refresh the page and the "Hello, World" page should still appear.

Django is now running purely within a Docker container. We are not working within a virtual environment locally. We did not execute the runserver command. All of our code now exists and our Django server is running within a self-contained Docker container. Success!

We will create multiple Docker images and containers over the course of this book and with practice the flow will start to make more sense:

- create a `Dockerfile` with custom image instructions
- add a `.dockerignore` file
- build the image
- create a `docker-compose.yml` file
- spin up the container(s)

Stop the currently running container with `<C-c>` and additionally type `docker-compose down`. Docker containers take up a lot of memory so it's a good idea to stop them when you’re done using them. Containers are meant to be stateless which is why we use volumes to copy our code over locally where it can be saved.

Whenever any new technology is introduced there are potential security concerns. In Docker's case, one example is that it sets the default user to root. The root user (also known as the "superuser" or "admin") is a special user account used in Linux for system administration. It is the most privileged user on a Linux system and has access to all commands and files.

The Docker docs contain a section a large section on Security and specifically on rootless mode to avoid this. We will not be covering it here since this is a book on Django, not Docker, but especially if your website stores sensitive information do review the entire [Security section](https://docs.docker.com/engine/security/rootless/) closely before going live.











